### **Importing Libraries**
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
%matplotlib inline
from sklearn.metrics import confusion_matrix,accuracy_score,make_scorer
from sklearn.model_selection import cross_validate
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score , ConfusionMatrixDisplay
## Exploratory Data Analysis (EDA)

## Diabetes Dataset
df=pd.read_csv('diabetes.csv')
df.head()
### **reading dataset**
#lets describe the data
df.describe()
#infromation of dataset
df.info()
### **Taking Care of Missing Values**
#any null values
#not neccessary in above information we can see
df.isnull().sum()
### **Taking Care of Duplicate Values**
df_dup = df.duplicated().any()
df_dup
print("Number of duplicate rows in original DataFrame:", df.duplicated().sum())
### **Histogram**
#histogram
df.hist(bins=10,figsize=(10,10))
plt.show()
### **coreleation matrix**
#correlation

sns.heatmap(df.corr())
# we can see skin thickness,insulin,pregnencies and age are full independent to each other
#age and pregencies has negative correlation
### **Outlier Detection and Removal**
#lets count total outcome in each target 0 1
#0 means no diabeted
#1 means patient with diabtes
sns.countplot(y=df['Outcome'],palette='Set1')
sns.set(style="ticks")
sns.pairplot(df, hue="Outcome")
#box plot for outlier visualization
sns.set(style="whitegrid")
df.boxplot(figsize=(15,6))
#outlier remove

Q1=df.quantile(0.25)
Q3=df.quantile(0.75)
IQR=Q3-Q1

print("---Q1--- \n",Q1)
print("\n---Q3--- \n",Q3)
print("\n---IQR---\n",IQR)

#print((df < (Q1 - 1.5 * IQR))|(df > (Q3 + 1.5 * IQR)))

#outlier remove
df_out = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]
df.shape,df_out.shape
#more than 80 records deleted
#Scatter matrix after removing outlier
sns.set(style="ticks")
sns.pairplot(df_out, hue="Outcome")
plt.show()
#lets extract features and targets
X=df_out.drop(columns=['Outcome'])
y=df_out['Outcome']
### **Train Test Split**


#Splitting train test data 80 20 ratio

train_X,test_X,train_y,test_y=train_test_split(X,y,test_size=0.4)
train_X.shape,test_X.shape,train_y.shape,test_y.shape

### **Training the Model**
### **Logistic regression**

#Logistic regession
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(train_X,train_y)

#accuracy on training data
X_train_prediction = lr_model.predict(train_X)
lr_training_data_accuracy = accuracy_score(train_y, X_train_prediction)
# Accuracy on test data
X_test_prediction = lr_model.predict(test_X)
lr_test_data_accuracy = accuracy_score(test_y, X_test_prediction)

# Displaying Logistic Regression accuracies
print("Logistic Regression Accuracy:")
print("Training Accuracy:", lr_training_data_accuracy)
print("Test Accuracy:", lr_test_data_accuracy)
#Logistic Regression

acc=[]
roc=[]
y_pred=lr_model.predict(test_X)
#find accuracy
ac=accuracy_score(test_y,y_pred)
acc.append(ac)

#find the ROC_AOC curvea
rc=roc_auc_score(test_y,y_pred)
roc.append(rc)
print("\nAccuracy {0} ROC {1}".format(ac,rc))

# Generate predictions with the best model
pred_y = lr_model.predict(test_X)

# Create the confusion matrix
cm = confusion_matrix(test_y, pred_y)

ConfusionMatrixDisplay(confusion_matrix=cm).plot();


import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score


y_scores = lr_model.predict_proba(test_X)[:,1]  # Probabilities of positive class
fpr, tpr, _ = roc_curve(test_y, y_scores)
roc_auc = roc_auc_score(test_y, y_scores)

plt.figure(figsize=(8, 6))
lw = 2
plt.plot(fpr, tpr, color='Red',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

### **Support Vector Machine**
#SVM
svm_model=SVC(kernel='linear')
svm_model.fit(train_X,train_y)
#accuracy on training data
X_train_prediction = svm_model.predict(train_X)
svm_training_data_accuracy = accuracy_score(train_y, X_train_prediction)
# Accuracy on test data
X_test_prediction = svm_model.predict(test_X)
svm_test_data_accuracy = accuracy_score(test_y, X_test_prediction)

# Displaying  SVM accuracies
print("SVM Accuracy:")
print("Training Accuracy:", svm_training_data_accuracy)
print("Test Accuracy:", svm_test_data_accuracy)
#Support Vector Machine
y_pred=svm_model.predict(test_X)
#find accuracy
ac=accuracy_score(test_y,y_pred)
acc.append(ac)

#find the ROC_AOC curve
rc=roc_auc_score(test_y,y_pred)
roc.append(rc)
print("\nAccuracy {0} ROC {1}".format(ac,rc))

# Generate predictions with the best model
pred_y = svm_model.predict(test_X)

# Create the confusion matrix
cm = confusion_matrix(test_y, pred_y)

ConfusionMatrixDisplay(confusion_matrix=cm).plot();

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

y_scores_svm = svm_model.decision_function(test_X)  # Decision scores
fpr_svm, tpr_svm, _ = roc_curve(test_y, y_scores_svm)
roc_auc_svm = roc_auc_score(test_y, y_scores_svm)

plt.figure(figsize=(8, 6))  # Adjust width and height as needed
lw = 2
plt.plot(fpr_svm, tpr_svm, color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc_svm)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (SVM)')
plt.legend(loc="lower right")
plt.show()

### **Random Forest**
#Random Forest
rf_model = RandomForestClassifier(200)
rf_model.fit(train_X, train_y)

#accuracy on training data
X_train_prediction = rf_model.predict(train_X)
rf_training_data_accuracy = accuracy_score(train_y, X_train_prediction)
# Accuracy on test data
X_test_prediction = rf_model.predict(test_X)
rf_test_data_accuracy = accuracy_score(test_y, X_test_prediction)
# Displaying Random Forest accuracies
print("RF Accuracy:")
print("Training Accuracy:", rf_training_data_accuracy)
print("Test Accuracy:", rf_test_data_accuracy)
#Random forest
y_pred=rf_model.predict(test_X)
#find accuracy
ac=accuracy_score(test_y,y_pred)
acc.append(ac)

#find the ROC_AOC curve
rc=roc_auc_score(test_y,y_pred)
roc.append(rc)
print("\nAccuracy {0} ROC {1}".format(ac,rc))

# Generate predictions with the best model
pred_y = rf_model.predict(test_X)

# Create the confusion matrix
cm = confusion_matrix(test_y, pred_y)

ConfusionMatrixDisplay(confusion_matrix=cm).plot();
#display predicted values uncomment below line
#pd.DataFrame(data={'Actual':test_y,'Predicted':y_pred}).head()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Assuming rf_model is your trained Random Forest model
y_scores_rf = rf_model.predict_proba(test_X)[:, 1]  # Probability of positive class
fpr_rf, tpr_rf, _ = roc_curve(test_y, y_scores_rf)
roc_auc_rf = roc_auc_score(test_y, y_scores_rf)

plt.figure(figsize=(8, 6))
lw = 2
plt.plot(fpr_rf, tpr_rf, color='magenta',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc_rf)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (Random Forest)')
plt.legend(loc="lower right")
plt.show()

### **Accuracy Graph**
import matplotlib.pyplot as plt

# Data
algorithms = ['Logistic Regression', 'SVM', 'Random Forest']
accuracy_scores = acc[:3]
roc_auc_scores = roc[:3]

# Plotting accuracy scores
plt.figure(figsize=(10, 5))
bars_acc = plt.bar(algorithms, accuracy_scores, label='Accuracy')

# Adding percentage labels
for bar, acc_score in zip(bars_acc, accuracy_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{acc_score:.2%}',
             ha='center', va='bottom')

plt.ylabel('Accuracy Score')
plt.xlabel('Algorithms')
plt.title('Accuracy of Different Algorithms')
plt.show()

# Plotting ROC AUC scores
plt.figure(figsize=(10, 5))
bars_roc = plt.bar(algorithms, roc_auc_scores, label='ROC AUC')

# Adding percentage labels
for bar, roc_auc_score in zip(bars_roc, roc_auc_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{roc_auc_score:.2%}',
             ha='center', va='bottom')

plt.ylabel('ROC AUC')
plt.xlabel('Algorithms')
plt.title('ROC AUC of Different Algorithms')
plt.show()

input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = lr_model.predict(input_data_reshaped)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')
**save model**
import pickle
filename = 'trained_model.sav'
pickle.dump(svm_model, open(filename, 'wb'))
# loading the saved model
loaded_model = pickle.load(open('trained_model.sav', 'rb'))
input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = lr_model.predict(input_data_reshaped)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

